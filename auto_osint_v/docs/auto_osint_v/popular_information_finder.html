<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>auto_osint_v.popular_information_finder API documentation</title>
<meta name="description" content="Finds entities (information) that is popular amongst the potentially corroborating sources." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>auto_osint_v.popular_information_finder</code></h1>
</header>
<section id="section-intro">
<p>Finds entities (information) that is popular amongst the potentially corroborating sources.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Finds entities (information) that is popular amongst the potentially corroborating sources.
&#34;&#34;&#34;
import itertools
from multiprocessing import Pool, Manager
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm


class PopularInformationFinder:
    &#34;&#34;&#34;Finds the popular information amongst given sources

    Class that provides methods that get text from sources and compares the number of times a
    particular entity is mentioned.
    &#34;&#34;&#34;

    def __init__(self, file_handler_object, entity_processor_object):
        &#34;&#34;&#34;Initialises the PopularInformationFinder object.

        Args:
            file_handler_object: gives the class access to the file_handler object.
            entity_processor_object: gives the class access to the entity_processor object.
        &#34;&#34;&#34;
        # Lazy creation of class attribute.
        try:
            manager = getattr(type(self), &#39;manager&#39;)
        except AttributeError:
            manager = type(self).manager = Manager()
        self.entities = manager.dict()
        self.file_handler = file_handler_object
        self.entity_processor = entity_processor_object

    def get_text_process_entities(self, source):
        &#34;&#34;&#34;Gets the body text from each source using its URL.

        Uses requests and BeautifulSoup to retrieve and parse the webpage&#39;s HTML into a readable
        format for entity recognition.

        This method updates the &#39;self.sources&#39; dictionary.

        Args:
            source: the individual source from the dictionary of sources.

        Returns:
            A list of key-value pairs (tuples).
            Note: key-value pairs are required for the map function to construct a dictionary from.
        &#34;&#34;&#34;
        # define entities variable
        entities = []
        # define the url
        url = source[&#34;url&#34;]
        # set headers to try to avoid 403 errors
        headers = {
            &#39;User-Agent&#39;:
                &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) &#39;
                &#39;Chrome/112.0.0.0 Safari/537.36&#39;}
        # request the webpage. If source website timeout, return the current list of entities.
        try:
            response = requests.get(url, headers, timeout=5)
        except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError):
            return entities
        # check if we are wasting our time with a broken or inaccessible website
        try:
            response.raise_for_status()
        except requests.HTTPError:
            return entities
        # get the content type
        try:
            content_type = response.headers[&#39;Content-Type&#39;]
            # if xml use xml parser
            if content_type == &#34;text/xml&#34; or content_type == &#34;application/xml&#34;:
                # use xml parser
                soup = BeautifulSoup(response.text, &#34;xml&#34;)
            else:
                # parse using the lxml html parser
                soup = BeautifulSoup(response.text, &#34;lxml&#34;)
        except KeyError:
            # except on KeyError if no &#39;content-type&#39; header exists
            soup = BeautifulSoup(response.text, &#34;lxml&#34;)

        # kill all script and style elements
        for script in soup([&#34;script&#34;, &#34;style&#34;]):
            script.extract()  # rip it out

        # get text
        text = soup.get_text()
        # break into lines and remove leading and trailing space on each
        lines = (line.strip() for line in text.splitlines())
        # break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split(&#34;  &#34;))
        # drop blank lines
        text = &#39;\n&#39;.join(chunk for chunk in chunks if chunk)
        # split into list
        textlist = text.split(&#39;\n&#39;)

        if len(text) &lt;= 100000:
            # run the text through the entity processor. stores entities in namesake variable
            entities = self.entity_processor.get_entities_and_count(textlist, self.entities)

        return entities

    def find_entities(self, sources):
        &#34;&#34;&#34;Finds entities in the given text.

        Uses the same model for entity recognition in specific_entity_processor.

        Looks like we need to scrap wikipedia articles because they are too long.
        Articles over 100k characters are probably too long also.
        Most slowdowns here have been due to Russia&#39;s wikipedia page.

        Args:
            sources: list of dictionaries of sources with corresponding URL.

        Returns:
            A list of the most popular words amongst all the sources.
        &#34;&#34;&#34;
        with Pool() as pool:
            # sources = tqdm(sources)  # add a progress bar
            # calculate an even chunksize for the imap function using pool size (max processes)
            chunksize = len(sources) / len(pool._pool)
            if int(chunksize) &lt; chunksize:
                chunksize = int(chunksize) + 1
            else:
                chunksize = int(chunksize)
            tmp = tqdm(pool.imap_unordered(self.get_text_process_entities, sources, chunksize),
                       total=len(sources))
            self.entities.update([tpl for sublist in tmp for tpl in sublist if tpl])

        # sort list of dictionaries by highest no. of mentions.
        # lambda function specifies sorted to use the values of the dictionary in desc. order
        sorted_entities = sorted(self.entities.items(), key=lambda x: x[1], reverse=True)
        # keep top 10% of popular entities, and no greater than 30 entities.
        cut_off_index = len(sorted_entities) * 0.10
        cut_off_index = int(min(cut_off_index, 30))
        # truncate the list based on cut_off_index
        sorted_entities = itertools.islice(sorted_entities, cut_off_index)
        sorted_entities_words = list(word for (word, count) in sorted_entities)

        return sorted_entities_words</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="auto_osint_v.popular_information_finder.PopularInformationFinder"><code class="flex name class">
<span>class <span class="ident">PopularInformationFinder</span></span>
<span>(</span><span>file_handler_object, entity_processor_object)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the popular information amongst given sources</p>
<p>Class that provides methods that get text from sources and compares the number of times a
particular entity is mentioned.</p>
<p>Initialises the PopularInformationFinder object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_handler_object</code></strong></dt>
<dd>gives the class access to the file_handler object.</dd>
<dt><strong><code>entity_processor_object</code></strong></dt>
<dd>gives the class access to the entity_processor object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PopularInformationFinder:
    &#34;&#34;&#34;Finds the popular information amongst given sources

    Class that provides methods that get text from sources and compares the number of times a
    particular entity is mentioned.
    &#34;&#34;&#34;

    def __init__(self, file_handler_object, entity_processor_object):
        &#34;&#34;&#34;Initialises the PopularInformationFinder object.

        Args:
            file_handler_object: gives the class access to the file_handler object.
            entity_processor_object: gives the class access to the entity_processor object.
        &#34;&#34;&#34;
        # Lazy creation of class attribute.
        try:
            manager = getattr(type(self), &#39;manager&#39;)
        except AttributeError:
            manager = type(self).manager = Manager()
        self.entities = manager.dict()
        self.file_handler = file_handler_object
        self.entity_processor = entity_processor_object

    def get_text_process_entities(self, source):
        &#34;&#34;&#34;Gets the body text from each source using its URL.

        Uses requests and BeautifulSoup to retrieve and parse the webpage&#39;s HTML into a readable
        format for entity recognition.

        This method updates the &#39;self.sources&#39; dictionary.

        Args:
            source: the individual source from the dictionary of sources.

        Returns:
            A list of key-value pairs (tuples).
            Note: key-value pairs are required for the map function to construct a dictionary from.
        &#34;&#34;&#34;
        # define entities variable
        entities = []
        # define the url
        url = source[&#34;url&#34;]
        # set headers to try to avoid 403 errors
        headers = {
            &#39;User-Agent&#39;:
                &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) &#39;
                &#39;Chrome/112.0.0.0 Safari/537.36&#39;}
        # request the webpage. If source website timeout, return the current list of entities.
        try:
            response = requests.get(url, headers, timeout=5)
        except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError):
            return entities
        # check if we are wasting our time with a broken or inaccessible website
        try:
            response.raise_for_status()
        except requests.HTTPError:
            return entities
        # get the content type
        try:
            content_type = response.headers[&#39;Content-Type&#39;]
            # if xml use xml parser
            if content_type == &#34;text/xml&#34; or content_type == &#34;application/xml&#34;:
                # use xml parser
                soup = BeautifulSoup(response.text, &#34;xml&#34;)
            else:
                # parse using the lxml html parser
                soup = BeautifulSoup(response.text, &#34;lxml&#34;)
        except KeyError:
            # except on KeyError if no &#39;content-type&#39; header exists
            soup = BeautifulSoup(response.text, &#34;lxml&#34;)

        # kill all script and style elements
        for script in soup([&#34;script&#34;, &#34;style&#34;]):
            script.extract()  # rip it out

        # get text
        text = soup.get_text()
        # break into lines and remove leading and trailing space on each
        lines = (line.strip() for line in text.splitlines())
        # break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split(&#34;  &#34;))
        # drop blank lines
        text = &#39;\n&#39;.join(chunk for chunk in chunks if chunk)
        # split into list
        textlist = text.split(&#39;\n&#39;)

        if len(text) &lt;= 100000:
            # run the text through the entity processor. stores entities in namesake variable
            entities = self.entity_processor.get_entities_and_count(textlist, self.entities)

        return entities

    def find_entities(self, sources):
        &#34;&#34;&#34;Finds entities in the given text.

        Uses the same model for entity recognition in specific_entity_processor.

        Looks like we need to scrap wikipedia articles because they are too long.
        Articles over 100k characters are probably too long also.
        Most slowdowns here have been due to Russia&#39;s wikipedia page.

        Args:
            sources: list of dictionaries of sources with corresponding URL.

        Returns:
            A list of the most popular words amongst all the sources.
        &#34;&#34;&#34;
        with Pool() as pool:
            # sources = tqdm(sources)  # add a progress bar
            # calculate an even chunksize for the imap function using pool size (max processes)
            chunksize = len(sources) / len(pool._pool)
            if int(chunksize) &lt; chunksize:
                chunksize = int(chunksize) + 1
            else:
                chunksize = int(chunksize)
            tmp = tqdm(pool.imap_unordered(self.get_text_process_entities, sources, chunksize),
                       total=len(sources))
            self.entities.update([tpl for sublist in tmp for tpl in sublist if tpl])

        # sort list of dictionaries by highest no. of mentions.
        # lambda function specifies sorted to use the values of the dictionary in desc. order
        sorted_entities = sorted(self.entities.items(), key=lambda x: x[1], reverse=True)
        # keep top 10% of popular entities, and no greater than 30 entities.
        cut_off_index = len(sorted_entities) * 0.10
        cut_off_index = int(min(cut_off_index, 30))
        # truncate the list based on cut_off_index
        sorted_entities = itertools.islice(sorted_entities, cut_off_index)
        sorted_entities_words = list(word for (word, count) in sorted_entities)

        return sorted_entities_words</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="auto_osint_v.popular_information_finder.PopularInformationFinder.find_entities"><code class="name flex">
<span>def <span class="ident">find_entities</span></span>(<span>self, sources)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds entities in the given text.</p>
<p>Uses the same model for entity recognition in specific_entity_processor.</p>
<p>Looks like we need to scrap wikipedia articles because they are too long.
Articles over 100k characters are probably too long also.
Most slowdowns here have been due to Russia's wikipedia page.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sources</code></strong></dt>
<dd>list of dictionaries of sources with corresponding URL.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A list of the most popular words amongst all the sources.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_entities(self, sources):
    &#34;&#34;&#34;Finds entities in the given text.

    Uses the same model for entity recognition in specific_entity_processor.

    Looks like we need to scrap wikipedia articles because they are too long.
    Articles over 100k characters are probably too long also.
    Most slowdowns here have been due to Russia&#39;s wikipedia page.

    Args:
        sources: list of dictionaries of sources with corresponding URL.

    Returns:
        A list of the most popular words amongst all the sources.
    &#34;&#34;&#34;
    with Pool() as pool:
        # sources = tqdm(sources)  # add a progress bar
        # calculate an even chunksize for the imap function using pool size (max processes)
        chunksize = len(sources) / len(pool._pool)
        if int(chunksize) &lt; chunksize:
            chunksize = int(chunksize) + 1
        else:
            chunksize = int(chunksize)
        tmp = tqdm(pool.imap_unordered(self.get_text_process_entities, sources, chunksize),
                   total=len(sources))
        self.entities.update([tpl for sublist in tmp for tpl in sublist if tpl])

    # sort list of dictionaries by highest no. of mentions.
    # lambda function specifies sorted to use the values of the dictionary in desc. order
    sorted_entities = sorted(self.entities.items(), key=lambda x: x[1], reverse=True)
    # keep top 10% of popular entities, and no greater than 30 entities.
    cut_off_index = len(sorted_entities) * 0.10
    cut_off_index = int(min(cut_off_index, 30))
    # truncate the list based on cut_off_index
    sorted_entities = itertools.islice(sorted_entities, cut_off_index)
    sorted_entities_words = list(word for (word, count) in sorted_entities)

    return sorted_entities_words</code></pre>
</details>
</dd>
<dt id="auto_osint_v.popular_information_finder.PopularInformationFinder.get_text_process_entities"><code class="name flex">
<span>def <span class="ident">get_text_process_entities</span></span>(<span>self, source)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the body text from each source using its URL.</p>
<p>Uses requests and BeautifulSoup to retrieve and parse the webpage's HTML into a readable
format for entity recognition.</p>
<p>This method updates the 'self.sources' dictionary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source</code></strong></dt>
<dd>the individual source from the dictionary of sources.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>A list of key-value pairs (tuples).</dt>
<dt><code>Note</code></dt>
<dd>key-value pairs are required for the map function to construct a dictionary from.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_text_process_entities(self, source):
    &#34;&#34;&#34;Gets the body text from each source using its URL.

    Uses requests and BeautifulSoup to retrieve and parse the webpage&#39;s HTML into a readable
    format for entity recognition.

    This method updates the &#39;self.sources&#39; dictionary.

    Args:
        source: the individual source from the dictionary of sources.

    Returns:
        A list of key-value pairs (tuples).
        Note: key-value pairs are required for the map function to construct a dictionary from.
    &#34;&#34;&#34;
    # define entities variable
    entities = []
    # define the url
    url = source[&#34;url&#34;]
    # set headers to try to avoid 403 errors
    headers = {
        &#39;User-Agent&#39;:
            &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) &#39;
            &#39;Chrome/112.0.0.0 Safari/537.36&#39;}
    # request the webpage. If source website timeout, return the current list of entities.
    try:
        response = requests.get(url, headers, timeout=5)
    except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError):
        return entities
    # check if we are wasting our time with a broken or inaccessible website
    try:
        response.raise_for_status()
    except requests.HTTPError:
        return entities
    # get the content type
    try:
        content_type = response.headers[&#39;Content-Type&#39;]
        # if xml use xml parser
        if content_type == &#34;text/xml&#34; or content_type == &#34;application/xml&#34;:
            # use xml parser
            soup = BeautifulSoup(response.text, &#34;xml&#34;)
        else:
            # parse using the lxml html parser
            soup = BeautifulSoup(response.text, &#34;lxml&#34;)
    except KeyError:
        # except on KeyError if no &#39;content-type&#39; header exists
        soup = BeautifulSoup(response.text, &#34;lxml&#34;)

    # kill all script and style elements
    for script in soup([&#34;script&#34;, &#34;style&#34;]):
        script.extract()  # rip it out

    # get text
    text = soup.get_text()
    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split(&#34;  &#34;))
    # drop blank lines
    text = &#39;\n&#39;.join(chunk for chunk in chunks if chunk)
    # split into list
    textlist = text.split(&#39;\n&#39;)

    if len(text) &lt;= 100000:
        # run the text through the entity processor. stores entities in namesake variable
        entities = self.entity_processor.get_entities_and_count(textlist, self.entities)

    return entities</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="auto_osint_v" href="index.html">auto_osint_v</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="auto_osint_v.popular_information_finder.PopularInformationFinder" href="#auto_osint_v.popular_information_finder.PopularInformationFinder">PopularInformationFinder</a></code></h4>
<ul class="">
<li><code><a title="auto_osint_v.popular_information_finder.PopularInformationFinder.find_entities" href="#auto_osint_v.popular_information_finder.PopularInformationFinder.find_entities">find_entities</a></code></li>
<li><code><a title="auto_osint_v.popular_information_finder.PopularInformationFinder.get_text_process_entities" href="#auto_osint_v.popular_information_finder.PopularInformationFinder.get_text_process_entities">get_text_process_entities</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>